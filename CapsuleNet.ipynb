{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "2c3412a4-039b-4229-9387-9321169f16eb",
    "_uuid": "8def9627e9d48b26de3159fc9a2ec38e854ab16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion-mnist_test.csv\n",
      "fashion-mnist_train.csv\n",
      "t10k-images-idx3-ubyte\n",
      "t10k-labels-idx1-ubyte\n",
      "train-images-idx3-ubyte\n",
      "train-labels-idx1-ubyte\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"data\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c56a778-ba24-4d51-a89d-a16b7da3f47c",
    "_uuid": "a04a2a1268aabc722a26e5aada6921afd8b19e2f"
   },
   "source": [
    "# Brief Info\n",
    "\n",
    "In this work, we will train a CNN classifier using Keras with the guidelines described in [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python).\n",
    "\n",
    "Our strategy will be using 20% of the train data (12000 data rows) as a validation set to optimize the classifier, while keeping test data to finally evaluate the accuracy of the model on the data it has never seen.\n",
    "\n",
    "#### Note\n",
    "Since I was not sure if the data was already shuffled, I didn't pass `validation_split=0.2` to _fit()_ and instead explicitly shuffled and split the validation data, as `validation_split` [would](https://keras.io/getting-started/faq/#how-is-the-validation-split-computed) use last 20% of the data in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "dec05004-ccb3-490e-b588-27c0f4f06d1e",
    "_uuid": "41907ec74cae883fa8d56f6556cade5c67c8f3e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train = pd.read_csv('data/fashion-mnist_train.csv')\n",
    "data_test = pd.read_csv('data/fashion-mnist_test.csv')\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X = np.array(data_train.iloc[:, 1:])\n",
    "y = to_categorical(np.array(data_train.iloc[:, 0]))\n",
    "\n",
    "#Here we split validation data to optimiza classifier during training\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "#Test data\n",
    "X_test = np.array(data_test.iloc[:, 1:])\n",
    "y_test = to_categorical(np.array(data_test.iloc[:, 0]))\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "0599166d-b975-4c88-8b91-071a8f4fb0cd",
    "_uuid": "0e9db73157e2c0e481bf3d73892d8d29263aa56f"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "\n",
    "#input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "4543df54-7373-49f8-a23a-6d8062eefe38",
    "_uuid": "9de535d8a446b7168ac3790445610425527c8a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 241,546\n",
      "Trainable params: 241,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c9d072ca-23e7-4608-a9d8-dc818715d6e3",
    "_uuid": "1e85d4b1d5fb567f2aad6bd82da969629e585f14"
   },
   "source": [
    "### Training\n",
    "Let's `fit()`! Note that `fit()` will return a _History_ object which we can use to plot training vs. validation accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f24bf149-70e5-43f2-8574-117606493c85",
    "_uuid": "71b754f661ab2bba5e0d2c280d033a57300ef7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 68s 1ms/step - loss: 0.8433 - acc: 0.6850 - val_loss: 0.5277 - val_acc: 0.8066\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 65s 1ms/step - loss: 0.5359 - acc: 0.7978 - val_loss: 0.4414 - val_acc: 0.8400\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 65s 1ms/step - loss: 0.4629 - acc: 0.8297 - val_loss: 0.3893 - val_acc: 0.8566\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.4171 - acc: 0.8476 - val_loss: 0.3490 - val_acc: 0.8723\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.3863 - acc: 0.8592 - val_loss: 0.3288 - val_acc: 0.8769\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 58s 1ms/step - loss: 0.3579 - acc: 0.8682 - val_loss: 0.3073 - val_acc: 0.8893\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.3441 - acc: 0.8736 - val_loss: 0.2979 - val_acc: 0.8916\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 69s 1ms/step - loss: 0.3258 - acc: 0.8807 - val_loss: 0.2850 - val_acc: 0.8954\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 61s 1ms/step - loss: 0.3167 - acc: 0.8842 - val_loss: 0.2813 - val_acc: 0.8966\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 63s 1ms/step - loss: 0.3037 - acc: 0.8885 - val_loss: 0.2701 - val_acc: 0.8991\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 67s 1ms/step - loss: 0.2953 - acc: 0.8925 - val_loss: 0.2642 - val_acc: 0.9016\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.2904 - acc: 0.8937 - val_loss: 0.2649 - val_acc: 0.9032\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2803 - acc: 0.8960 - val_loss: 0.2627 - val_acc: 0.9002\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 61s 1ms/step - loss: 0.2708 - acc: 0.9005 - val_loss: 0.2545 - val_acc: 0.9062\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.2721 - acc: 0.8999 - val_loss: 0.2644 - val_acc: 0.9021\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.2631 - acc: 0.9015 - val_loss: 0.2488 - val_acc: 0.9068\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2595 - acc: 0.9030 - val_loss: 0.2488 - val_acc: 0.9068\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 67s 1ms/step - loss: 0.2561 - acc: 0.9041 - val_loss: 0.2451 - val_acc: 0.9088\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 61s 1ms/step - loss: 0.2492 - acc: 0.9074 - val_loss: 0.2431 - val_acc: 0.9092\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.2471 - acc: 0.9081 - val_loss: 0.2388 - val_acc: 0.9087\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 67s 1ms/step - loss: 0.2433 - acc: 0.9085 - val_loss: 0.2394 - val_acc: 0.9122\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 59s 1ms/step - loss: 0.2420 - acc: 0.9096 - val_loss: 0.2351 - val_acc: 0.9112\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.2374 - acc: 0.9128 - val_loss: 0.2386 - val_acc: 0.9087\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2355 - acc: 0.9113 - val_loss: 0.2369 - val_acc: 0.9123\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 58s 1ms/step - loss: 0.2297 - acc: 0.9147 - val_loss: 0.2339 - val_acc: 0.9106\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 58s 1ms/step - loss: 0.2302 - acc: 0.9135 - val_loss: 0.2264 - val_acc: 0.9167\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2223 - acc: 0.9165 - val_loss: 0.2315 - val_acc: 0.9148\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 61s 1ms/step - loss: 0.2195 - acc: 0.9174 - val_loss: 0.2327 - val_acc: 0.9128\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2189 - acc: 0.9174 - val_loss: 0.2257 - val_acc: 0.9143\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.2180 - acc: 0.9180 - val_loss: 0.2295 - val_acc: 0.9137\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2174 - acc: 0.9198 - val_loss: 0.2253 - val_acc: 0.9157\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 69s 1ms/step - loss: 0.2119 - acc: 0.9203 - val_loss: 0.2270 - val_acc: 0.9164\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2135 - acc: 0.9199 - val_loss: 0.2247 - val_acc: 0.9181\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2080 - acc: 0.9220 - val_loss: 0.2249 - val_acc: 0.9182\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 74s 2ms/step - loss: 0.2146 - acc: 0.9191 - val_loss: 0.2251 - val_acc: 0.9159\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 79s 2ms/step - loss: 0.2085 - acc: 0.9214 - val_loss: 0.2230 - val_acc: 0.9193\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 79s 2ms/step - loss: 0.2062 - acc: 0.9224 - val_loss: 0.2297 - val_acc: 0.9145\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 74s 2ms/step - loss: 0.2045 - acc: 0.9215 - val_loss: 0.2233 - val_acc: 0.9182\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 76s 2ms/step - loss: 0.2033 - acc: 0.9247 - val_loss: 0.2260 - val_acc: 0.9163\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 61s 1ms/step - loss: 0.2027 - acc: 0.9237 - val_loss: 0.2215 - val_acc: 0.9200\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 59s 1ms/step - loss: 0.1970 - acc: 0.9242 - val_loss: 0.2227 - val_acc: 0.9180\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 59s 1ms/step - loss: 0.1979 - acc: 0.9259 - val_loss: 0.2281 - val_acc: 0.9168\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 57s 1ms/step - loss: 0.1948 - acc: 0.9266 - val_loss: 0.2206 - val_acc: 0.9188\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 57s 1ms/step - loss: 0.1958 - acc: 0.9266 - val_loss: 0.2209 - val_acc: 0.9182\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 61s 1ms/step - loss: 0.1966 - acc: 0.9263 - val_loss: 0.2213 - val_acc: 0.9177\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.1911 - acc: 0.9283 - val_loss: 0.2260 - val_acc: 0.9182\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.1898 - acc: 0.9266 - val_loss: 0.2196 - val_acc: 0.9199\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.1894 - acc: 0.9279 - val_loss: 0.2246 - val_acc: 0.9183\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 58s 1ms/step - loss: 0.1885 - acc: 0.9289 - val_loss: 0.2191 - val_acc: 0.9218\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 62s 1ms/step - loss: 0.1875 - acc: 0.9289 - val_loss: 0.2205 - val_acc: 0.9198\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "model.save(\"capsulenet_on_fashion_minst.h5\")\n",
    "score = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f4b6cb78-1ef8-46ba-b72d-49a49c81bff0",
    "_uuid": "48787728c57ad402547540b5d528aabbd8b20c0d"
   },
   "outputs": [],
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f860c207-af4d-4c32-b4d2-4ea2d39902fe",
    "_uuid": "d18bc5dd0ec23cda8ea5ad846b9877704f484951"
   },
   "source": [
    "### Results\n",
    "It turns out our classifier does better then the best baseline reported [here](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/), which is an SVM classifier with mean accuracy of 0.897.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "00dc5434-a008-490d-bb89-a394cf95c4bc",
    "_uuid": "adbb96b3d90dcab280f06341a8483f246f583a03"
   },
   "source": [
    "\n",
    "Let's plot training and validation accuracy as well as loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "324c3a68-6381-4a39-a5a1-8fac766150c0",
    "_uuid": "042117c73a10d7cbda6bc42ab60f9b0407cdf2c1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "accuracy = history.history['acc']\n",
    "val_accuracy = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1311601b-9a53-4e59-adcd-4365c811f85e",
    "_uuid": "28e5331842de7baaffb3a1e74f6a4c03e140759a"
   },
   "source": [
    "### Classification Report\n",
    "We can summarize the performance of our classifier as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "721514ef-520b-42a2-9d57-2f82fbae8393",
    "_uuid": "0275d1189e6425353537fa3ebe5d97d7b6759551"
   },
   "outputs": [],
   "source": [
    "#get the predictions for the test data\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "#get the indices to be plotted\n",
    "y_true = data_test.iloc[:, 0]\n",
    "correct = np.nonzero(predicted_classes==y_true)[0]\n",
    "incorrect = np.nonzero(predicted_classes!=y_true)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b131353-2dd2-4799-b4aa-9fd02a698ccd",
    "_uuid": "0ca5eabf7daa083bedc21b7a7f6f31daade91e6e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_true, predicted_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc36576e-c630-4581-89fe-3bed02c378bb",
    "_uuid": "2863d680886cc22a2e6f11270b411e435410adaf"
   },
   "source": [
    "It's apparent that our classifier is underperforming for class 6 in terms of both precision and recall. For class 2, classifier is slightly lacking precision whereas it is slightly lacking recall (i.e. missed) for class 4.\n",
    "\n",
    "Perhaps we would gain more insight after visualizing the correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "866ef2af-d4e1-4697-bf87-9dcaa7cba363",
    "_uuid": "6a78901cefbe9a3bdf89cc1d22ee910970cfaf93"
   },
   "source": [
    "Here is a subset of correctly predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a33d457-ff62-4e32-81d7-89a6cdc1106d",
    "_uuid": "d8148b3557ac27d216e0137bbeae06379c829779",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, correct in enumerate(correct[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_true[correct]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9726e897-4d69-44d5-a1d5-358d316b5141",
    "_uuid": "a22f4456fa1c09e609db1a6c0e8eb1438087fa91"
   },
   "source": [
    "And here is a subset of incorrectly predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "803d7c3f-2099-420d-9612-dcf617d6cbe0",
    "_uuid": "fa4d360a4e77bd3d83490065351860b1d9f58f8b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, incorrect in enumerate(incorrect[0:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_true[incorrect]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd66b1b1-db6f-47e4-aa2e-3ce516979fa3",
    "_uuid": "89881c6e4d1169ac543b5de992832d9e423dfdde"
   },
   "source": [
    "It looks like diversity of the similar patterns present on multiple classes effect the performance of the classifier although CNN is a robust architechture. A jacket, a shirt, and a long-sleeve blouse has similar patterns: long sleeves (or not!), buttons (or not!), and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2d4e41bf-07a3-4aa1-a89b-b2b1c3c19aaf",
    "_uuid": "04ce0bbb3160fe7750432a20a912308598cfc804"
   },
   "source": [
    "#### What did the network learn?\n",
    "\n",
    "The snippets are taken from _Chollet, F (2017)_. The idea is the give an input data and visualize the activations of the conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2f9993fe-e0b8-4850-84aa-92499afc9b60",
    "_uuid": "382e3bd0030f728700ce0a6c3e2e9442d641c6e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_im = X_train[13]\n",
    "plt.imshow(test_im.reshape(28,28), cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e836de6d-4378-40ce-a3b3-f01269788479",
    "_uuid": "ca607572942ddc3080e4b8b74d6bc47779fe66ae"
   },
   "source": [
    "Let's see the activation of the 2nd channel of the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d3952d67-8187-4a8e-9e82-8bef6482e04c",
    "_uuid": "15a26cf258d1fd6152d201258538ce829bed1ccc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "activation_model = models.Model(input=model.input, output=layer_outputs)\n",
    "activations = activation_model.predict(test_im.reshape(1,28,28,1))\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "plt.matshow(first_layer_activation[0, :, :, 4], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6081331c-280d-442f-afc2-edf3c54b02bd",
    "_uuid": "1969b931bf29620f2546454de5133e3b2df8080f"
   },
   "source": [
    "Let's plot the activations of the other conv layers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a9e0c8cc-6b25-4043-8f7c-d8b95ec27923",
    "_uuid": "0ac1180af62085842f361291ff716bdb3935c98a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "for layer in model.layers[:-1]:\n",
    "    if isinstance(layer, Conv2D):\n",
    "        layer_names.append(layer.name)\n",
    "images_per_row = 16\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "    n_cols = n_features / images_per_row\n",
    "    n_cols = int(n_cols)\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,:, :, col * images_per_row + row]\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='bone')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
